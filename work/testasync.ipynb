{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71c82ce0-392a-4dae-bf6d-ac50084b4834",
   "metadata": {},
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9776ad7e-2099-49b3-a4f8-c6baea6f43ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "# load_dotenv()\n",
    "# API_KEY = os.getenv(\"OPENWEATHER_API_KEY\")\n",
    "# API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4420c756-9533-4e93-9486-cabc314efba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TEST\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pytz\n",
    "from datetime import timedelta\n",
    "\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from prefect import flow, task # Prefect flow and task decorators\n",
    "from math import ceil\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"OPENWEATHER_API_KEY\")\n",
    "ACCESS_KEY = os.getenv(\"LAKEFS_ACCESS_KEY\")\n",
    "SECRET_KEY = os.getenv(\"LAKEFS_SECRET_KEY\")\n",
    "lakefs_endpoint = os.getenv(\"LAKEFS_ENDPOINT\", \"http://lakefs-dev:8000\")\n",
    "\n",
    "@task\n",
    "async def fetch_pollution_data(df_sample, batch_size=100):  #sample\n",
    "    # API_KEY = \"13fd31c548dbeae77a5c0d773a99df79\"\n",
    "    # API_KEY = os.getenv(\"OPENWEATHER_API_KEY\")\n",
    "    # WEATHER_ENDPOINT = \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "    POLLUTION_ENDPOINT = \"http://api.openweathermap.org/data/2.5/air_pollution\"\n",
    "    \n",
    "    async def fetch_row(session, row):\n",
    "        # await asyncio.sleep(1)  #พัก 1 วิแบบไม่บล็อก loop\n",
    "        lat = row['lat']\n",
    "        lon = row['lon']\n",
    "        province = row['provinceEN']\n",
    "        district = row['amphoeEN']\n",
    "        try:\n",
    "            params = {\n",
    "                \"lat\" : lat,\n",
    "                \"lon\" : lon,\n",
    "                \"appid\": API_KEY,\n",
    "                \"units\": \"metric\"\n",
    "            }\n",
    "            async with session.get(POLLUTION_ENDPOINT, params=params) as response:\n",
    "                # if response.status != 200:\n",
    "                #     return {\n",
    "                #         'province': province,\n",
    "                #         'amphoe': amphoe,\n",
    "                #         'lat': lat,\n",
    "                #         'lon': lon,\n",
    "                #         'error': f\"HTTP {response.status}\"\n",
    "                #     }\n",
    "                data = await response.json()\n",
    "                dt = datetime.utcnow()\n",
    "                thai_tz = pytz.timezone('Asia/Bangkok')\n",
    "                localtime = dt.astimezone(thai_tz)\n",
    "                components = data['list'][0]['components']\n",
    "                pollution_dict = {\n",
    "                    'timestamp': dt,\n",
    "                    'year': dt.year,\n",
    "                    'month': dt.month,\n",
    "                    'day': dt.day,\n",
    "                    'hour': dt.hour,\n",
    "                    'minute': dt.minute,\n",
    "                    'localtime': localtime,\n",
    "                    'province' : province,\n",
    "                    'district' : district,\n",
    "                    'lat' : data['coord']['lat'],\n",
    "                    'lon' : data['coord']['lon'],\n",
    "                    # 'location': data['name'],\n",
    "                    'main.aqi' : data['list'][0]['main']['aqi'],\n",
    "                    'components_co' : components['co'],\n",
    "                    'components_no' : components['no'],\n",
    "                    'components_no2' : components['no2'],\n",
    "                    'components_o3' : components['o3'],\n",
    "                    'components_so2' : components['so2'],\n",
    "                    'components_pm2_5' : components['pm2_5'],\n",
    "                    'components_pm10' : components['pm10'],\n",
    "                    'components_nh3' : components['nh3']\n",
    "                    }\n",
    "                return pollution_dict\n",
    "\n",
    "        # # Check Error\n",
    "        # except Exception as e:\n",
    "        #     return {\n",
    "        #         'province': row.get('province', 'unknown'),\n",
    "        #         'amphoe': row.get('amphoe', 'unknown'),\n",
    "        #         'lat': row.get('lat', None),\n",
    "        #         'lon': row.get('lon', None),\n",
    "        #         'error': str(e)\n",
    "        # }\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching data: {e}\")\n",
    "            return None\n",
    "        except KeyError as e:\n",
    "            print(f\"Error processing data: Missing key {e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e} at {province} - {district}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "    pollution_results = []\n",
    "    total_batches = ceil(len(df_sample) / batch_size)  #sample\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for i in range(total_batches):\n",
    "            batch = df_sample.iloc[i*batch_size:(i+1)*batch_size]\n",
    "            tasks = [fetch_row(session, row) for _, row in batch.iterrows()]\n",
    "            batch_results = await asyncio.gather(*tasks)\n",
    "            pollution_results.extend(batch_results)\n",
    "\n",
    "            print(f\"✅ เสร็จ batch {i+1}/{total_batches}\")\n",
    "            if i < total_batches - 1:\n",
    "                await asyncio.sleep(65)  # ✅ รอให้ผ่าน rate limit\n",
    "\n",
    "    return pollution_results\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    df = pd.DataFrame(df)\n",
    "    # if 'error' in df.columns:\n",
    "    #     # df = df[df['error'].isna()]  # กรอง row ที่มี error ออก\n",
    "    #     df = df.drop(columns=['error'], errors='ignore')  # ลบคอลัมน์ error\n",
    "    df['province'] = df['province'].astype(\"string\")\n",
    "    df['district'] = df['district'].astype(\"string\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_to_lakefs(df):\n",
    "    # ACCESS_KEY = os.getenv(\"LAKEFS_ACCESS_KEY\")\n",
    "    # SECRET_KEY = os.getenv(\"LAKEFS_SECRET_KEY\")\n",
    "    # lakefs_endpoint = os.getenv(\"LAKEFS_ENDPOINT\", \"http://lakefs-dev:8000\")\n",
    "    # ACCESS_KEY = \"access_key\"\n",
    "    # SECRET_KEY = \"secret_key\"\n",
    "    # lakefs_endpoint = \"http://lakefs-dev:8000/\"\n",
    "\n",
    "    repo = \"pollution-data\"\n",
    "    branch = \"main\"\n",
    "    path = \"pollution.parquet\"\n",
    "    \n",
    "    lakefs_s3_path = f\"s3a://{repo}/{branch}/{path}\"\n",
    "\n",
    "    storage_options = {\n",
    "        \"key\": ACCESS_KEY,\n",
    "        \"secret\": SECRET_KEY,\n",
    "        \"client_kwargs\": {\"endpoint_url\": lakefs_endpoint}\n",
    "    }\n",
    "\n",
    "    df.to_parquet(\n",
    "        lakefs_s3_path,\n",
    "        storage_options=storage_options,\n",
    "        partition_cols=[\"year\", \"month\", \"day\", \"hour\"],\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b888c73-e131-4a6d-9dc4-179ef795ff81",
   "metadata": {},
   "source": [
    "### FLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f1ca1ec-84f3-4fe5-94d8-d44e4a3742f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "@flow(name=\"pollution-flow\", log_prints=True)\n",
    "async def pollution_flow():\n",
    "    start_time = time.perf_counter()  # ⏱ เริ่มจับเวลา\n",
    "\n",
    "    BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  #/home/jovyan/work\n",
    "    coord_path = os.path.join(BASE_DIR, \"save\", \"amphoe_coord.csv\")\n",
    "    coord_df = pd.read_csv(coord_path)\n",
    "    df_sample = coord_df.sample(10)\n",
    "    pollution_results = await fetch_pollution_data(df_sample)\n",
    "    # pollution_data = pd.DataFrame(pollution_results)\n",
    "\n",
    "    # #Check Error\n",
    "    # df_raw = pd.DataFrame(pollution_results)\n",
    "    # if 'error' in df_raw.columns:\n",
    "    #     error_df = df_raw[df_raw['error'].notna()]\n",
    "    #     print(\"\\n❗ Error Rows:\")\n",
    "    #     print(error_df[['province', 'amphoe', 'error']].head())\n",
    "    pollution_data = clean_data(pollution_results)\n",
    "    \n",
    "    \n",
    "    end_time = time.perf_counter()  # ⏱ จับเวลาอีกครั้ง\n",
    "    print(f\"\\n✅ ดึงข้อมูลเสร็จทั้งหมด ใช้เวลา {end_time - start_time:.2f} วินาที\")\n",
    "    save_to_lakefs(pollution_data) \n",
    "    print(\"save to Lakefs Success\")\n",
    "    print(pollution_data.info())\n",
    "    print(pollution_data.head())\n",
    "    # return pollution_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5dcb80d-bb7d-4a39-a700-6f1ceb478059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>province_id</th>\n",
       "      <th>province</th>\n",
       "      <th>provinceEN</th>\n",
       "      <th>amphoe_id</th>\n",
       "      <th>amphoe</th>\n",
       "      <th>amphoeEN</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>46</td>\n",
       "      <td>กาฬสินธุ์</td>\n",
       "      <td>Kalasin</td>\n",
       "      <td>4614</td>\n",
       "      <td>ห้วยผึ้ง</td>\n",
       "      <td>Huai Phueng</td>\n",
       "      <td>16.652100</td>\n",
       "      <td>103.891200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>30</td>\n",
       "      <td>นครราชสีมา</td>\n",
       "      <td>Nakhon Ratchasima</td>\n",
       "      <td>3004</td>\n",
       "      <td>คง</td>\n",
       "      <td>Khong</td>\n",
       "      <td>15.422308</td>\n",
       "      <td>102.419010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>90</td>\n",
       "      <td>สงขลา</td>\n",
       "      <td>Songkhla</td>\n",
       "      <td>9005</td>\n",
       "      <td>เทพา</td>\n",
       "      <td>Thepha</td>\n",
       "      <td>6.797260</td>\n",
       "      <td>100.910250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>86</td>\n",
       "      <td>ชุมพร</td>\n",
       "      <td>Chumphon</td>\n",
       "      <td>8606</td>\n",
       "      <td>พะโต๊ะ</td>\n",
       "      <td>Phato</td>\n",
       "      <td>9.791667</td>\n",
       "      <td>98.776667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>66</td>\n",
       "      <td>พิจิตร</td>\n",
       "      <td>Phichit</td>\n",
       "      <td>6605</td>\n",
       "      <td>บางมูลนาก</td>\n",
       "      <td>Bang Mun Nak</td>\n",
       "      <td>16.021400</td>\n",
       "      <td>100.417200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>10</td>\n",
       "      <td>กรุงเทพมหานคร</td>\n",
       "      <td>Bangkok</td>\n",
       "      <td>1012</td>\n",
       "      <td>ยานนาวา</td>\n",
       "      <td>Yan Nawa</td>\n",
       "      <td>13.694300</td>\n",
       "      <td>100.539400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>21</td>\n",
       "      <td>ระยอง</td>\n",
       "      <td>Rayong</td>\n",
       "      <td>2108</td>\n",
       "      <td>นิคมพัฒนา</td>\n",
       "      <td>Nikhom Phatthana</td>\n",
       "      <td>14.909170</td>\n",
       "      <td>101.026730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>63</td>\n",
       "      <td>ตาก</td>\n",
       "      <td>Tak</td>\n",
       "      <td>6303</td>\n",
       "      <td>สามเงา</td>\n",
       "      <td>Sam Ngao</td>\n",
       "      <td>17.243310</td>\n",
       "      <td>99.022560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>55</td>\n",
       "      <td>น่าน</td>\n",
       "      <td>Nan</td>\n",
       "      <td>5505</td>\n",
       "      <td>ปัว</td>\n",
       "      <td>Pua</td>\n",
       "      <td>19.175000</td>\n",
       "      <td>100.916000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>71</td>\n",
       "      <td>กาญจนบุรี</td>\n",
       "      <td>Kanchanaburi</td>\n",
       "      <td>7101</td>\n",
       "      <td>เมืองกาญจนบุรี</td>\n",
       "      <td>Mueang Kanchanaburi</td>\n",
       "      <td>14.069870</td>\n",
       "      <td>99.327690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     province_id       province         provinceEN  amphoe_id          amphoe  \\\n",
       "77            46      กาฬสินธุ์            Kalasin       4614        ห้วยผึ้ง   \n",
       "262           30     นครราชสีมา  Nakhon Ratchasima       3004              คง   \n",
       "626           90          สงขลา           Songkhla       9005            เทพา   \n",
       "183           86          ชุมพร           Chumphon       8606          พะโต๊ะ   \n",
       "456           66         พิจิตร            Phichit       6605       บางมูลนาก   \n",
       "57            10  กรุงเทพมหานคร            Bangkok       1012         ยานนาวา   \n",
       "518           21          ระยอง             Rayong       2108       นิคมพัฒนา   \n",
       "209           63            ตาก                Tak       6303          สามเงา   \n",
       "329           55           น่าน                Nan       5505             ปัว   \n",
       "59            71      กาญจนบุรี       Kanchanaburi       7101  เมืองกาญจนบุรี   \n",
       "\n",
       "                amphoeEN        lat         lon  \n",
       "77           Huai Phueng  16.652100  103.891200  \n",
       "262                Khong  15.422308  102.419010  \n",
       "626               Thepha   6.797260  100.910250  \n",
       "183                Phato   9.791667   98.776667  \n",
       "456         Bang Mun Nak  16.021400  100.417200  \n",
       "57              Yan Nawa  13.694300  100.539400  \n",
       "518     Nikhom Phatthana  14.909170  101.026730  \n",
       "209             Sam Ngao  17.243310   99.022560  \n",
       "329                  Pua  19.175000  100.916000  \n",
       "59   Mueang Kanchanaburi  14.069870   99.327690  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #Check path + Sample\n",
    "# BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  #/home/jovyan/work\n",
    "# coord_path = os.path.join(BASE_DIR, \"save\", \"amphoe_coord.csv\")\n",
    "# coord_df = pd.read_csv(coord_path)\n",
    "# df_sample = coord_df.sample(10)\n",
    "# df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e59b1ae-068c-4b94-b63f-88ae8f8eafd1",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5394fee3-d882-4108-bc94-3d285e991b38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">11:27:15.347 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | prefect.engine - Created flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'tough-squid'</span> for flow<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> 'pollution-flow'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "11:27:15.347 | \u001b[36mINFO\u001b[0m    | prefect.engine - Created flow run\u001b[35m 'tough-squid'\u001b[0m for flow\u001b[1;35m 'pollution-flow'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">11:27:15.354 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | prefect.engine - View at <span style=\"color: #0000ff; text-decoration-color: #0000ff\">http://prefect-server:4200/runs/flow-run/4851c040-3c79-4ec0-8d69-b48a5b309790</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "11:27:15.354 | \u001b[36mINFO\u001b[0m    | prefect.engine - View at \u001b[94mhttp://prefect-server:4200/runs/flow-run/4851c040-3c79-4ec0-8d69-b48a5b309790\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">11:27:15.618 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'fetch_pollution_data-0' - Created task run 'fetch_pollution_data-0' for task 'fetch_pollution_data'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "11:27:15.618 | \u001b[36mINFO\u001b[0m    | Task run 'fetch_pollution_data-0' - Created task run 'fetch_pollution_data-0' for task 'fetch_pollution_data'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">11:27:15.997 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'fetch_pollution_data-0' - ✅ เสร็จ batch 1/1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "11:27:15.997 | \u001b[36mINFO\u001b[0m    | Task run 'fetch_pollution_data-0' - ✅ เสร็จ batch 1/1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">11:27:16.044 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'fetch_pollution_data-0' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "11:27:16.044 | \u001b[36mINFO\u001b[0m    | Task run 'fetch_pollution_data-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">11:27:16.051 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'tough-squid'</span> - \n",
       "✅ ดึงข้อมูลเสร็จทั้งหมด ใช้เวลา 0.61 วินาที\n",
       "</pre>\n"
      ],
      "text/plain": [
       "11:27:16.051 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'tough-squid'\u001b[0m - \n",
       "✅ ดึงข้อมูลเสร็จทั้งหมด ใช้เวลา 0.61 วินาที\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">11:27:16.081 | <span style=\"color: #d70000; text-decoration-color: #d70000\">ERROR</span>   | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'tough-squid'</span> - Encountered exception during execution: PermissionError('Access Denied.')\n",
       "Traceback (most recent call last):\n",
       "  File \"/usr/local/lib/python3.11/site-packages/s3fs/core.py\", line 114, in _error_wrapper\n",
       "    return await func(*args, **kwargs)\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/usr/local/lib/python3.11/site-packages/aiobotocore/client.py\", line 412, in _make_api_call\n",
       "    raise error_class(parsed_response, operation_name)\n",
       "botocore.exceptions.ClientError: An error occurred (AccessDenied) when calling the CreateBucket operation: Access Denied.\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "Traceback (most recent call last):\n",
       "  File \"/usr/local/lib/python3.11/site-packages/prefect/flow_engine.py\", line 636, in run_context\n",
       "    yield self\n",
       "  File \"/usr/local/lib/python3.11/site-packages/prefect/flow_engine.py\", line 699, in run_flow_async\n",
       "    await engine.call_flow_fn()\n",
       "  File \"/usr/local/lib/python3.11/site-packages/prefect/flow_engine.py\", line 654, in _call_flow_fn\n",
       "    result = await call_with_parameters(self.flow.fn, self.parameters)\n",
       "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/tmp/ipykernel_1709/712246882.py\", line 27, in pollution_flow\n",
       "    save_to_lakefs(pollution_data)\n",
       "  File \"/tmp/ipykernel_1709/2025065849.py\", line 152, in save_to_lakefs\n",
       "    df.to_parquet(\n",
       "  File \"/usr/local/lib/python3.11/site-packages/pandas/util/_decorators.py\", line 333, in wrapper\n",
       "    return func(*args, **kwargs)\n",
       "           ^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/usr/local/lib/python3.11/site-packages/pandas/core/frame.py\", line 3113, in to_parquet\n",
       "    return to_parquet(\n",
       "           ^^^^^^^^^^^\n",
       "  File \"/usr/local/lib/python3.11/site-packages/pandas/io/parquet.py\", line 480, in to_parquet\n",
       "    impl.write(\n",
       "  File \"/usr/local/lib/python3.11/site-packages/pandas/io/parquet.py\", line 218, in write\n",
       "    self.api.parquet.write_to_dataset(\n",
       "  File \"/usr/local/lib/python3.11/site-packages/pyarrow/parquet/core.py\", line 2172, in write_to_dataset\n",
       "    ds.write_dataset(\n",
       "  File \"/usr/local/lib/python3.11/site-packages/pyarrow/dataset.py\", line 1034, in write_dataset\n",
       "    _filesystemdataset_write(\n",
       "  File \"pyarrow/_dataset.pyx\", line 4071, in pyarrow._dataset._filesystemdataset_write\n",
       "  File \"pyarrow/error.pxi\", line 89, in pyarrow.lib.check_status\n",
       "  File \"pyarrow/_fs.pyx\", line 1526, in pyarrow._fs._cb_create_dir\n",
       "  File \"/usr/local/lib/python3.11/site-packages/pyarrow/fs.py\", line 361, in create_dir\n",
       "    self.fs.mkdir(path, create_parents=recursive)\n",
       "  File \"/usr/local/lib/python3.11/site-packages/fsspec/asyn.py\", line 118, in wrapper\n",
       "    return sync(self.loop, func, *args, **kwargs)\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/usr/local/lib/python3.11/site-packages/fsspec/asyn.py\", line 103, in sync\n",
       "    raise return_result\n",
       "  File \"/usr/local/lib/python3.11/site-packages/fsspec/asyn.py\", line 56, in _runner\n",
       "    result[0] = await coro\n",
       "                ^^^^^^^^^^\n",
       "  File \"/usr/local/lib/python3.11/site-packages/s3fs/core.py\", line 954, in _mkdir\n",
       "    await self._call_s3(\"create_bucket\", **params)\n",
       "  File \"/usr/local/lib/python3.11/site-packages/s3fs/core.py\", line 371, in _call_s3\n",
       "    return await _error_wrapper(\n",
       "           ^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/usr/local/lib/python3.11/site-packages/s3fs/core.py\", line 146, in _error_wrapper\n",
       "    raise err\n",
       "PermissionError: Access Denied.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "11:27:16.081 | \u001b[38;5;160mERROR\u001b[0m   | Flow run\u001b[35m 'tough-squid'\u001b[0m - Encountered exception during execution: PermissionError('Access Denied.')\n",
       "Traceback (most recent call last):\n",
       "  File \"/usr/local/lib/python3.11/site-packages/s3fs/core.py\", line 114, in _error_wrapper\n",
       "    return await func(*args, **kwargs)\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/usr/local/lib/python3.11/site-packages/aiobotocore/client.py\", line 412, in _make_api_call\n",
       "    raise error_class(parsed_response, operation_name)\n",
       "botocore.exceptions.ClientError: An error occurred (AccessDenied) when calling the CreateBucket operation: Access Denied.\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "Traceback (most recent call last):\n",
       "  File \"/usr/local/lib/python3.11/site-packages/prefect/flow_engine.py\", line 636, in run_context\n",
       "    yield self\n",
       "  File \"/usr/local/lib/python3.11/site-packages/prefect/flow_engine.py\", line 699, in run_flow_async\n",
       "    await engine.call_flow_fn()\n",
       "  File \"/usr/local/lib/python3.11/site-packages/prefect/flow_engine.py\", line 654, in _call_flow_fn\n",
       "    result = await call_with_parameters(self.flow.fn, self.parameters)\n",
       "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/tmp/ipykernel_1709/712246882.py\", line 27, in pollution_flow\n",
       "    save_to_lakefs(pollution_data)\n",
       "  File \"/tmp/ipykernel_1709/2025065849.py\", line 152, in save_to_lakefs\n",
       "    df.to_parquet(\n",
       "  File \"/usr/local/lib/python3.11/site-packages/pandas/util/_decorators.py\", line 333, in wrapper\n",
       "    return func(*args, **kwargs)\n",
       "           ^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/usr/local/lib/python3.11/site-packages/pandas/core/frame.py\", line 3113, in to_parquet\n",
       "    return to_parquet(\n",
       "           ^^^^^^^^^^^\n",
       "  File \"/usr/local/lib/python3.11/site-packages/pandas/io/parquet.py\", line 480, in to_parquet\n",
       "    impl.write(\n",
       "  File \"/usr/local/lib/python3.11/site-packages/pandas/io/parquet.py\", line 218, in write\n",
       "    self.api.parquet.write_to_dataset(\n",
       "  File \"/usr/local/lib/python3.11/site-packages/pyarrow/parquet/core.py\", line 2172, in write_to_dataset\n",
       "    ds.write_dataset(\n",
       "  File \"/usr/local/lib/python3.11/site-packages/pyarrow/dataset.py\", line 1034, in write_dataset\n",
       "    _filesystemdataset_write(\n",
       "  File \"pyarrow/_dataset.pyx\", line 4071, in pyarrow._dataset._filesystemdataset_write\n",
       "  File \"pyarrow/error.pxi\", line 89, in pyarrow.lib.check_status\n",
       "  File \"pyarrow/_fs.pyx\", line 1526, in pyarrow._fs._cb_create_dir\n",
       "  File \"/usr/local/lib/python3.11/site-packages/pyarrow/fs.py\", line 361, in create_dir\n",
       "    self.fs.mkdir(path, create_parents=recursive)\n",
       "  File \"/usr/local/lib/python3.11/site-packages/fsspec/asyn.py\", line 118, in wrapper\n",
       "    return sync(self.loop, func, *args, **kwargs)\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/usr/local/lib/python3.11/site-packages/fsspec/asyn.py\", line 103, in sync\n",
       "    raise return_result\n",
       "  File \"/usr/local/lib/python3.11/site-packages/fsspec/asyn.py\", line 56, in _runner\n",
       "    result[0] = await coro\n",
       "                ^^^^^^^^^^\n",
       "  File \"/usr/local/lib/python3.11/site-packages/s3fs/core.py\", line 954, in _mkdir\n",
       "    await self._call_s3(\"create_bucket\", **params)\n",
       "  File \"/usr/local/lib/python3.11/site-packages/s3fs/core.py\", line 371, in _call_s3\n",
       "    return await _error_wrapper(\n",
       "           ^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/usr/local/lib/python3.11/site-packages/s3fs/core.py\", line 146, in _error_wrapper\n",
       "    raise err\n",
       "PermissionError: Access Denied.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">11:27:16.118 | <span style=\"color: #d70000; text-decoration-color: #d70000\">ERROR</span>   | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'tough-squid'</span> - Finished in state <span style=\"color: #d70000; text-decoration-color: #d70000\">Failed</span>('Flow run encountered an exception: PermissionError: Access Denied.')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "11:27:16.118 | \u001b[38;5;160mERROR\u001b[0m   | Flow run\u001b[35m 'tough-squid'\u001b[0m - Finished in state \u001b[38;5;160mFailed\u001b[0m('Flow run encountered an exception: PermissionError: Access Denied.')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "PermissionError",
     "evalue": "Access Denied.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/s3fs/core.py:114\u001b[39m, in \u001b[36m_error_wrapper\u001b[39m\u001b[34m(func, args, kwargs, retries)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m S3_RETRYABLE_ERRORS \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/aiobotocore/client.py:412\u001b[39m, in \u001b[36mAioBaseClient._make_api_call\u001b[39m\u001b[34m(self, operation_name, api_params)\u001b[39m\n\u001b[32m    411\u001b[39m     error_class = \u001b[38;5;28mself\u001b[39m.exceptions.from_code(error_code)\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mClientError\u001b[39m: An error occurred (AccessDenied) when calling the CreateBucket operation: Access Denied.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mnest_asyncio\u001b[39;00m\n\u001b[32m      3\u001b[39m nest_asyncio.apply()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m pollution_flow()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/prefect/flow_engine.py:701\u001b[39m, in \u001b[36mrun_flow_async\u001b[39m\u001b[34m(flow, flow_run, parameters, wait_for, return_type)\u001b[39m\n\u001b[32m    698\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m engine.run_context():\n\u001b[32m    699\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m engine.call_flow_fn()\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m engine.state \u001b[38;5;28;01mif\u001b[39;00m return_type == \u001b[33m\"\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/prefect/flow_engine.py:246\u001b[39m, in \u001b[36mFlowRunEngine.result\u001b[39m\u001b[34m(self, raise_on_failure)\u001b[39m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._raised \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m NotSet:\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_failure:\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._raised\n\u001b[32m    247\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._raised\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# This is a fall through case which leans on the existing state result mechanics to get the\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# return value. This is necessary because we currently will return a State object if the\u001b[39;00m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# the State was Prefect-created.\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;66;03m# TODO: Remove the need to get the result from a State except in cases where the return value\u001b[39;00m\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# is a State object.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/prefect/flow_engine.py:636\u001b[39m, in \u001b[36mFlowRunEngine.run_context\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    629\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m timeout_context(\n\u001b[32m    630\u001b[39m         seconds=\u001b[38;5;28mself\u001b[39m.flow.timeout_seconds,\n\u001b[32m    631\u001b[39m         timeout_exc_type=FlowRunTimeoutError,\n\u001b[32m    632\u001b[39m     ):\n\u001b[32m    633\u001b[39m         \u001b[38;5;28mself\u001b[39m.logger.debug(\n\u001b[32m    634\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExecuting flow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.flow.name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m for flow run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.flow_run.name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    635\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m636\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    638\u001b[39m     \u001b[38;5;28mself\u001b[39m.handle_timeout(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/prefect/flow_engine.py:699\u001b[39m, in \u001b[36mrun_flow_async\u001b[39m\u001b[34m(flow, flow_run, parameters, wait_for, return_type)\u001b[39m\n\u001b[32m    697\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m engine.is_running():\n\u001b[32m    698\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m engine.run_context():\n\u001b[32m--> \u001b[39m\u001b[32m699\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m engine.call_flow_fn()\n\u001b[32m    701\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m engine.state \u001b[38;5;28;01mif\u001b[39;00m return_type == \u001b[33m\"\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m engine.result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/prefect/flow_engine.py:654\u001b[39m, in \u001b[36mFlowRunEngine.call_flow_fn.<locals>._call_flow_fn\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    653\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_call_flow_fn\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m654\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m call_with_parameters(\u001b[38;5;28mself\u001b[39m.flow.fn, \u001b[38;5;28mself\u001b[39m.parameters)\n\u001b[32m    655\u001b[39m     \u001b[38;5;28mself\u001b[39m.handle_success(result)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mpollution_flow\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     25\u001b[39m end_time = time.perf_counter()  \u001b[38;5;66;03m# ⏱ จับเวลาอีกครั้ง\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ ดึงข้อมูลเสร็จทั้งหมด ใช้เวลา \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m วินาที\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43msave_to_lakefs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpollution_data\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33msave to Lakefs Success\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(pollution_data.info())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 152\u001b[39m, in \u001b[36msave_to_lakefs\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m    144\u001b[39m lakefs_s3_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33ms3a://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbranch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    146\u001b[39m storage_options = {\n\u001b[32m    147\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mkey\u001b[39m\u001b[33m\"\u001b[39m: ACCESS_KEY,\n\u001b[32m    148\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msecret\u001b[39m\u001b[33m\"\u001b[39m: SECRET_KEY,\n\u001b[32m    149\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mclient_kwargs\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mendpoint_url\u001b[39m\u001b[33m\"\u001b[39m: lakefs_endpoint}\n\u001b[32m    150\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlakefs_s3_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43myear\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmonth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mday\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhour\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pandas/core/frame.py:3113\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[39m\n\u001b[32m   3032\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3033\u001b[39m \u001b[33;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[32m   3034\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3109\u001b[39m \u001b[33;03m>>> content = f.read()\u001b[39;00m\n\u001b[32m   3110\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3111\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3113\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3114\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3121\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3122\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pandas/io/parquet.py:480\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m impl = get_engine(engine)\n\u001b[32m    478\u001b[39m path_or_buf: FilePath | WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] = io.BytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io.BytesIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pandas/io/parquet.py:218\u001b[39m, in \u001b[36mPyArrowImpl.write\u001b[39m\u001b[34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m partition_cols \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    217\u001b[39m         \u001b[38;5;66;03m# writes to multiple files under the given path\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_to_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    226\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    227\u001b[39m         \u001b[38;5;66;03m# write to single output file\u001b[39;00m\n\u001b[32m    228\u001b[39m         \u001b[38;5;28mself\u001b[39m.api.parquet.write_table(\n\u001b[32m    229\u001b[39m             table,\n\u001b[32m    230\u001b[39m             path_or_handle,\n\u001b[32m   (...)\u001b[39m\u001b[32m    233\u001b[39m             **kwargs,\n\u001b[32m    234\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyarrow/parquet/core.py:2172\u001b[39m, in \u001b[36mwrite_to_dataset\u001b[39m\u001b[34m(table, root_path, partition_cols, filesystem, use_legacy_dataset, schema, partitioning, basename_template, use_threads, file_visitor, existing_data_behavior, **kwargs)\u001b[39m\n\u001b[32m   2169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m existing_data_behavior \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2170\u001b[39m     existing_data_behavior = \u001b[33m'\u001b[39m\u001b[33moverwrite_or_ignore\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2172\u001b[39m \u001b[43mds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2174\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mparquet_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_visitor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_visitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbasename_template\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbasename_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexisting_data_behavior\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexisting_data_behavior\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2179\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mwrite_dataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyarrow/dataset.py:1034\u001b[39m, in \u001b[36mwrite_dataset\u001b[39m\u001b[34m(data, base_dir, basename_template, format, partitioning, partitioning_flavor, schema, filesystem, file_options, use_threads, max_partitions, max_open_files, max_rows_per_file, min_rows_per_group, max_rows_per_group, file_visitor, existing_data_behavior, create_dir)\u001b[39m\n\u001b[32m   1031\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot specify a schema when writing a Scanner\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1032\u001b[39m     scanner = data\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m \u001b[43m_filesystemdataset_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscanner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasename_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_partitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_visitor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexisting_data_behavior\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1037\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_open_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows_per_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_rows_per_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows_per_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_dir\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyarrow/_dataset.pyx:4071\u001b[39m, in \u001b[36mpyarrow._dataset._filesystemdataset_write\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyarrow/error.pxi:89\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyarrow/_fs.pyx:1526\u001b[39m, in \u001b[36mpyarrow._fs._cb_create_dir\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyarrow/fs.py:361\u001b[39m, in \u001b[36mFSSpecHandler.create_dir\u001b[39m\u001b[34m(self, path, recursive)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mcreate_dir\u001b[39m(\u001b[38;5;28mself\u001b[39m, path, recursive):\n\u001b[32m    359\u001b[39m     \u001b[38;5;66;03m# mkdir also raises FileNotFoundError when base directory is not found\u001b[39;00m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_parents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[32m    363\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/fsspec/asyn.py:118\u001b[39m, in \u001b[36msync_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    117\u001b[39m     \u001b[38;5;28mself\u001b[39m = obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/fsspec/asyn.py:103\u001b[39m, in \u001b[36msync\u001b[39m\u001b[34m(loop, func, timeout, *args, **kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m FSTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mreturn_result\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m return_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/fsspec/asyn.py:56\u001b[39m, in \u001b[36m_runner\u001b[39m\u001b[34m(event, coro, result, timeout)\u001b[39m\n\u001b[32m     54\u001b[39m     coro = asyncio.wait_for(coro, timeout=timeout)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     result[\u001b[32m0\u001b[39m] = \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m     58\u001b[39m     result[\u001b[32m0\u001b[39m] = ex\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/s3fs/core.py:954\u001b[39m, in \u001b[36mS3FileSystem._mkdir\u001b[39m\u001b[34m(self, path, acl, create_parents, **kwargs)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m region_name:\n\u001b[32m    951\u001b[39m     params[\u001b[33m\"\u001b[39m\u001b[33mCreateBucketConfiguration\u001b[39m\u001b[33m\"\u001b[39m] = {\n\u001b[32m    952\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mLocationConstraint\u001b[39m\u001b[33m\"\u001b[39m: region_name\n\u001b[32m    953\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m954\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_s3(\u001b[33m\"\u001b[39m\u001b[33mcreate_bucket\u001b[39m\u001b[33m\"\u001b[39m, **params)\n\u001b[32m    955\u001b[39m \u001b[38;5;28mself\u001b[39m.invalidate_cache(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    956\u001b[39m \u001b[38;5;28mself\u001b[39m.invalidate_cache(bucket)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/s3fs/core.py:371\u001b[39m, in \u001b[36mS3FileSystem._call_s3\u001b[39m\u001b[34m(self, method, *akwarglist, **kwargs)\u001b[39m\n\u001b[32m    369\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mCALL: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, method.\u001b[34m__name__\u001b[39m, akwarglist, kw2)\n\u001b[32m    370\u001b[39m additional_kwargs = \u001b[38;5;28mself\u001b[39m._get_s3_method_kwargs(method, *akwarglist, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _error_wrapper(\n\u001b[32m    372\u001b[39m     method, kwargs=additional_kwargs, retries=\u001b[38;5;28mself\u001b[39m.retries\n\u001b[32m    373\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/s3fs/core.py:146\u001b[39m, in \u001b[36m_error_wrapper\u001b[39m\u001b[34m(func, args, kwargs, retries)\u001b[39m\n\u001b[32m    144\u001b[39m         err = e\n\u001b[32m    145\u001b[39m err = translate_boto_error(err)\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "\u001b[31mPermissionError\u001b[39m: Access Denied."
     ]
    }
   ],
   "source": [
    "# test-run pollution flow\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "await pollution_flow()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092a2077-7ba2-4591-82c2-f27598f1bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(results):\n",
    "    # pollution_result = pd.DataFrame(results)\n",
    "    pollution_result = pollution_result[pollution_result['error'].isna()]  # กรอง error ออก\n",
    "    return pollution_result.drop(columns=['error'], errors='ignore')\n",
    "\n",
    "# แยก clean กับ error ออก\n",
    "clean_results = [r for r in results if 'error' not in r]\n",
    "error_results = [r for r in results if 'error' in r]\n",
    "\n",
    "# สร้าง DataFrame\n",
    "clean_df = pd.DataFrame(clean_results)\n",
    "error_df = pd.DataFrame(error_results)\n",
    "\n",
    "# Save ไฟล์ clean ขึ้น LakeFS (หรือ local เตรียมอัป)\n",
    "clean_df.to_parquet(\"pollution_data.parquet\", index=False)\n",
    "\n",
    "# เก็บ error ไว้ดูภายหลัง (optional)\n",
    "if not error_df.empty:\n",
    "    error_df.to_csv(\"pollution_errors.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904b585f-051b-4441-a7df-ef75622353af",
   "metadata": {},
   "source": [
    "### LakeFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2ca67b-f080-49ec-b733-ceca7b4058e6",
   "metadata": {},
   "outputs": [],
   "source": [
    " # lakeFS credentials from your docker-compose.yml\n",
    "ACCESS_KEY = \"access_key\"\n",
    "SECRET_KEY = \"secret_key\"\n",
    "    \n",
    "# lakeFS endpoint (running locally)\n",
    "lakefs_endpoint = \"http://lakefs-dev:8000/\"\n",
    "    \n",
    "    # lakeFS repository, branch, and file path\n",
    "repo = \"pollution-data\"\n",
    "branch = \"main\"\n",
    "path = \"pollution.parquet\"\n",
    "    \n",
    "    # Construct the full lakeFS S3-compatible path\n",
    "lakefs_s3_path = f\"s3a://{repo}/{branch}/{path}\"\n",
    "    \n",
    "    # Configure storage_options for lakeFS (S3-compatible)\n",
    "storage_options = {\n",
    "    \"key\": ACCESS_KEY,\n",
    "    \"secret\": SECRET_KEY,\n",
    "    \"client_kwargs\": {\n",
    "        \"endpoint_url\": lakefs_endpoint\n",
    "    }\n",
    "    }\n",
    "pollution_df.to_parquet(\n",
    "    lakefs_s3_path,\n",
    "    storage_options=storage_options,\n",
    "    partition_cols=['year','month','day','hour'],\n",
    "        \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6d2606-66ea-4c7b-b546-4d8f826df8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef08406-45c3-4058-a4a1-9b4591f88417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff3f6f8-9fb4-4462-acc9-03eed52787c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect import flow, task\n",
    "\n",
    "@task\n",
    "async def fetch_pollution_data(...):\n",
    "    ...\n",
    "\n",
    "def clean_data(results):\n",
    "    df = pd.DataFrame(results)\n",
    "    df = df[df['error'].isna()]  # กรอง error ออก\n",
    "    return df.drop(columns=['error'], errors='ignore')\n",
    "\n",
    "def save_to_lakefs(df, path):\n",
    "    df.to_parquet(path, index=False)\n",
    "\n",
    "@flow\n",
    "async def pollution_pipeline():\n",
    "    raw_results = await fetch_pollution_data(...)\n",
    "    clean_df = clean_data(raw_results)\n",
    "    save_to_lakefs(clean_df, \"s3://lakefs/bucket/pollution.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85bdc65-863d-43dd-82be-20821032c158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafdd5c8-7704-4f45-912d-614206baa52e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
